Tool Support for Testing

---------------------------------

Understanding Types of Test Tools

Purpose of Tools
- pros:
- - something that helps you with a process
- - perform complicated operation or repetitive talks
Cons:
- - replaces creativity and common-sense in problem solving
- - Automate more effort than the task
- improve the effectivity of the test activities, automation or because it's specialised for the task
- support manual tested eg. generating late amount of test data
- decrease chance of human error, increase testing consistency
- tester may not know hoe some activities are done manually
- increase reliability of testing

Probe Effect
- the effect on the component or system by the measurement instrument when the component or system is being measured
- present in performance, code coverage and debugging tools
- continual measurement of a system has -ve effects
- - response time is higher
- - slightly different coverage results as the source code and instrumentation code differ slightly
- ui test run slightly slower when debugging
- tools can be divide by pricing model, licence model, technology, purpose, activity support

Classifying test tools
- dived by test activity they support
- - management of testing and tester
- - static testing
- - test design and implementation
- - test execution and logging
- - performance measurement and dynamic analysis
- - Specialized testing needs
- some work on one activity, others many but are classified under their primary function

Tools for management of testing
- test-ware are the products generated by the tests process, eg. documentations, scripts, product
- Management of testing process
- - applicable throughout the entire software development life cycle
- - includes: test planning, execution, time tracking, budgeting
- management of test cases
- - management of test cases, its org, its version
- tools that provide many features are classified as test management tools
-  features:
- - management of tests and test activities
- - scheduling test execution and logging test results
- - linking tests, results, and defects to requirements or other sources
- - creating reports from various metrics such as number of tests failed, tests passed, defect raised etc
- usually interface to other tools, bug tracking, config, or execution tools
- management of test-ware:
- - requirement management, config management, defect management and continuous integration tools
- req management tools focus on traceability between requirements and test cases
- - you can track coverage, identify ambiguities  in requirement, enable prioritization and support reporting
- defect management tools (bug tracking) allow storage of defect info (description, env, severity), assign bugs to people,
- Configuration management tools help store version info, store traceability between software versions and test-ware versions assist in build and release management
- Continuous integrations tools, dev orientated and helps verify continuously changing code from any sources, gives feedback on the code always up to date

Tools for static testing
- manual or tool driven review of code without code execution
- used by devs to analyse code and report finding
- most IDE have basic tools to prevent bad syntax
- higher levels can calculate complexity, security flaws and potential bugs
- lowers risks and serves as a quality gate

Tools for test design and implementation
- QA testers have al ot of creative  freedom and critical thinking during this phase
- model based testing
- - derives and uses models for testing based on the system, often done with a tool
- - generating many cases, automation, identify gaps, improve communication, choosing between algorithms
- data preparation Tools
- - creates unique data and /or large amount of data
- - anonymize production data
- - also can do data sorting
- - performance testing would need a lot of unique data



tools for test execution and logging
- test execution tools
- - runs tests, may or may not require programming knowledge
- - tester record the test results as per the steps give
- - Pros: Automate simple tasks, understand the idea of test automation
- - cons: recorded tests are hard to maintain, little/no control of the state of the app, all info is hard coded
- Test execution, logging, comparing results to expected values, creating reports, taking snapshots of system
- coverage tools belong in tis category
- test harnesses are replacement s of real components of a system to isolate the testing to other components

tools for performance measuring amd dynamic analysis
- a testing tool is everything that assists with analysis of a system
- - Loading test - how a system copes with a large number of interactions
- - Volume test - how a system handle a lage amount of data
- - Stress test - verify how a system behave outside e expected limits
- performance test primary characteristics:
- -  generate loa and simulate scenarios
- - measure response time and other metrics
- - identify performance bottlenecks
- - produce visual reports
- Dynamic analysis tools:
- - memory leads, time dependencies, pointer arithmetic errors
- - require the code to be running
- Tools for specialised needs
- - eg performance tools specified for embedded systems, or dynamic analysis tools for security issues

----------------------------------

Benefits and risks of test automation


Identifying the benefits
- test execution time is shorted and coverage is increased
- reduces repetitive manual work
- provides greater consistency and repeatability
- gives objective assessment
- provides easier access to information about testing

employing a test automation tool
- how automation helps a company
- A new company needs to test thoroughly ove4r a large amount of product to catch up ot competition
- A company with frequent new features needs to shorten time t market
- reducing cost is appealing to any competitive company
- automations give a faster feedback cycle and early defect identification
- Times saving =cost saving
- high coverage
- higher accuracy
- additional feature unavailable to manual testing, eg large test data

Exploring the risks
- how automation can hinder a company
- purchasing a tool is not a guarantee for achieving benefits
- - each tool requires an investment of effort and time
- expectations fo the tool may be unrealistic
- initial introduction of tool takes time, cost and effort
- the time and effort to achieve notable and lasing benefits of a tool may be underestimated
- effort needed to maintain the test work product generated by the tool may be underestimated
- tool ma be relied on too much, manual testing ignored
- version control  may be neglected
- relationships and interoperability issues between critical tools may be neglected
- the tool vendor may go of business, retire the tool or sell it
- vendor may not provide adequate support, upgrades and defect fixes
- open-source tool may be abandoned
- new platform or tech may not support the tool
- no clear ownership of a tool question whose responsible

-------------------------------------

Selecting tools

Selecting tools for an org
- asses org maturity, strengths, weaknesses
- evaluate the tool features against clear requirements and objective criteria
- identify internal req for coaching and mentoring the use of the tool


main principles for tool selection
- identification of areas within the organization where tool support will help improve resting processes
- - do you want work done faster, or maybe more secure
- estimation of cost-benefit based on concrete business case
- Evaluation of vendor or open-source networks of support
- consider pros and cons of licencing modules
- - is the tool available for a trial period
- compatibility of tool with current tech used by the org
- compatibility and integration of the tool with a build
- - continuous integration tools already used in the org
- Evaluation of training needs
- - consider  the testing skills of those to be working with the tool

considerations for selecting test execution tools
- Test Execution tool: a test tool that executes tests against a designated test item and evaluates the outcome against expected results and post-conditions
- capturing test approach
- - Advantages: good starting point to get familiar with test automation, don't require programming knowledge or prior planning
- - Disadvantage: cannot scale to large number of tests, test data is hard-coded, tests are unstable when unexpected events occur, high maintenance
-  good for automating one-time tasks, not good for a part of your testing process
- data driven test approach - separate datafile for the test script, data can be easily changed
- - Advantages: possible to implement variation of scenario with one test script,
- - Disadvantage: requires specialized programming support, takes effort to set up, needs to be well-managed to provide benefits
- keyword-driven test approach (extension of data-driven)
- - Advantages: automation engine contains a control script and many support scripts that use keywords to import associated test data
- -  Additional tests can be implemented without increased the number of scripts, engineers without programming knowledge can also write test scripts
- - can be used to develop tool-independent automated tests, increase of changing tool vendor
- - Suitable for scaling to a large number of scripts
- - Disadvantage:

considerations for selecting test management tools
- how does a tool fir into the current management process
- - can the tool track the same parameters and generate reports in the same way the org current does
- - can the tool be integrated with currently used tools, and at what cost
- Are management tools needed for small organisations?
- - even small organizations should define their own process and check for a suitable tool
- - consider following the standards and procedures that are assumed by the way the tool works

----------------------------------------------

Introducing tools into an organisation

Introducing a tool
- use comprehensive selection criteria
- conduct a small scale study (proof of concept)

identifying objectives of a pilot project
- a sort of proof of concept
- set clear objectives before starting
- gaining in depth knowledge of the tool (strength and weaknesses)
- evaluation how the tool fits into the current practices and processes
- deciding on standard way of using and maintaining the tool and the test work products
- assessing weather the benefits will be achieved at cost
- understanding the metrics that you wish the tool to collect and report and configure the tool accordingly
- problems are expected, and solutions should be available for others later on
- pilot project can decide if the tool is useful or not, was the pilot project well run?

Success factors for a tool
- rolling out the tool for the rest of the org incrementally
- Adapting ad improving processes to fit with the use of the tool
- providing training, coaching, and mentoring for the tool users
- defining guidelines to the use of the tool
- implementing a way to gather usage information from the actual use of the tool
- monitoring the use and benefits
- providing support to the users of a given tool
- gathering lessons learned from all users























